{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação de módulos utilziados no projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "\n",
    "# Adicionando o diretório do projeto ao sys.path\n",
    "project_path = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "sys.path.append(project_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-processamento de dados\n",
    "\n",
    "Limpeza dos textos \n",
    "\n",
    "-- remoção de pontuação, números, caracteres especiais\n",
    "\n",
    "-- conversão para lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Função de limpeza de texto\n",
    "def preprocess_text(text):\n",
    "    # Remover pontuação, números e símbolos especiais\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../data/fake_and_real_news.csv')\n",
    "\n",
    "df_clean = pd.DataFrame({'Text': [], 'label': []})\n",
    "\n",
    "df_clean['Text'] = df_original['Text'].apply(preprocess_text)\n",
    "\n",
    "mapeamento = {'Real': 1, 'Fake': 0}\n",
    "   \n",
    "# cópia e escrita em disco da coluna 'label' normalizada para o dataframe que contém os dados limpos \n",
    "df_clean['label'] = df_original['label'].map(mapeamento)\n",
    "\n",
    "df_clean.to_csv('../cleaned_data/cleaned_texts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('../cleaned_data/cleaned_texts.csv')\n",
    "\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inicializando o TfidfVectorizer com 1000 features\n",
    "# pois o conjunto todo é muito grande e resulta em sobrecarga de memória \n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Processamento em partes menores (Chunking) para evitar sobrecarregar a memória.\n",
    "chunk_size = 1000 \n",
    "chunks = [df_clean['Text'][i:i + chunk_size] \n",
    "          for i in range(0, df_clean.shape[0], chunk_size)]\n",
    "\n",
    "# Cada parte é transformada usando o TfidfVectorizer.\n",
    "tfidf_chunks = [vectorizer.fit_transform(chunk) for chunk in chunks]\n",
    "x = vstack(tfidf_chunks)\n",
    "\n",
    "# Utilizando o MiniBatchKMeans, que é uma variação de K-means \n",
    "# projetada para trabalhar com grandes conjuntos de dados. \n",
    "# Ele processa os dados em mini-batches, reduzindo o uso de memória.\n",
    "num_clusters = 10\n",
    "kmeans = MiniBatchKMeans(n_clusters= num_clusters, \n",
    "                         random_state=0, batch_size=chunk_size).fit(x)\n",
    "\n",
    "# # Adicionando os rótulos dos clusters ao DataFrame original\n",
    "df_clean['Cluster'] = kmeans.labels_\n",
    "\n",
    "\n",
    "# Visualizando os clusters com PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x.toarray())\n",
    "\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1], \n",
    "            c=kmeans.labels_, cmap='rainbow')\n",
    "plt.title('Clusters de Notícias')\n",
    "plt.xlabel('Componente 1')\n",
    "plt.ylabel('Componente 2')\n",
    "# plt.savefig('../images/clusterizacao_num10.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rótulos únicos em y_teste: [0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carregar os dados\n",
    "df_clean = pd.read_csv('../cleaned_data/cleaned_texts.csv')\n",
    "\n",
    "# Separar as features e labels\n",
    "X = df_clean['Text'].copy()\n",
    "y = df_clean['label'].copy()\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converter o texto para vetores TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_treino_tfidf = vectorizer.fit_transform(x_treino)\n",
    "x_teste_tfidf = vectorizer.transform(x_teste)\n",
    "\n",
    "# Verificar os rótulos presentes em y_teste\n",
    "rotulos_unicos = y_teste.unique()\n",
    "print(f\"Rótulos únicos em y_teste: {rotulos_unicos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo Decision Tree\n",
    "modelo_dt = DecisionTreeClassifier(max_depth=10, min_samples_split=50, min_samples_leaf=10)\n",
    "modelo_dt.fit(x_treino_tfidf, y_treino)\n",
    "\n",
    "# Obter as características do TfidfVectorizer\n",
    "nomes_caracteristicas = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Avaliar o modelo Decision Tree\n",
    "y_teste_pred_dt = modelo_dt.predict(x_teste_tfidf)\n",
    "acuracia_dt = accuracy_score(y_teste, y_teste_pred_dt)\n",
    "precisao_dt = precision_score(y_teste, y_teste_pred_dt)\n",
    "recall_dt = recall_score(y_teste, y_teste_pred_dt)\n",
    "f1_dt = f1_score(y_teste, y_teste_pred_dt)\n",
    "\n",
    "print(f\"Acurácia Decision Tree: {acuracia_dt}\")\n",
    "print(f\"Precisão Decision Tree: {precisao_dt}\")\n",
    "print(f\"Recall Decision Tree: {recall_dt}\")\n",
    "print(f\"F1 Score Decision Tree: {f1_dt}\")\n",
    "\n",
    "# Matriz de Confusão Decision Tree\n",
    "cm_dt = confusion_matrix(y_teste, y_teste_pred_dt)\n",
    "disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=rotulos_unicos)\n",
    "disp_dt.plot(cmap='tab20b')\n",
    "plt.title('Matriz de Confusão para Decision Tree')\n",
    "# plt.savefig('../images/matriz_confusao_decision_tree.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar a árvore de decisão\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(modelo_dt, feature_names=nomes_caracteristicas, \n",
    "          class_names=['Real', 'Fake'], filled=True)\n",
    "plt.title('Árvore de Decisão com Palavras')\n",
    "plt.savefig('../images/arvore_decisao_com_palavras.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o modelo\n",
    "mod_nb = DecisionTreeClassifier()\n",
    "\n",
    "# Executar a validação cruzada\n",
    "cv_scores_nb = cross_val_score(mod_nb, x_treino_tfidf, y_treino, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores (Árvore de Decisão): {cv_scores_nb}\")\n",
    "print(f\"Mean CV Accuracy (Árvore de Decisão): {cv_scores_nb.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo Logistic Regression\n",
    "modelo_lr = LogisticRegression(max_iter=1000)\n",
    "modelo_lr.fit(x_treino_tfidf, y_treino)\n",
    "\n",
    "# Avaliar o modelo Logistic Regression\n",
    "y_teste_pred_lr = modelo_lr.predict(x_teste_tfidf)\n",
    "acuracia_lr = accuracy_score(y_teste, y_teste_pred_lr)\n",
    "precisao_lr = precision_score(y_teste, y_teste_pred_lr)\n",
    "recall_lr = recall_score(y_teste, y_teste_pred_lr)\n",
    "f1_lr = f1_score(y_teste, y_teste_pred_lr)\n",
    "\n",
    "print(f\"Acurácia Logistic Regression: {acuracia_lr}\")\n",
    "print(f\"Precisão Logistic Regression: {precisao_lr}\")\n",
    "print(f\"Recall Logistic Regression: {recall_lr}\")\n",
    "print(f\"F1 Score Logistic Regression: {f1_lr}\")\n",
    "\n",
    "# Matriz de Confusão Logistic Regression\n",
    "cm_lr = confusion_matrix(y_teste, y_teste_pred_lr)\n",
    "disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=rotulos_unicos)\n",
    "disp_lr.plot(cmap='tab20b')\n",
    "plt.title('Matriz de Confusão para Logistic Regression')\n",
    "plt.savefig('../images/matriz_confusao_logistic_regression.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o modelo\n",
    "mod_lr = LogisticRegression()\n",
    "\n",
    "# Executar a validação cruzada\n",
    "cv_scores_lr = cross_val_score(mod_lr, x_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores (Logistic Regression): {cv_scores_lr}\")\n",
    "print(f\"Mean CV Accuracy (Logistic Regression): {cv_scores_lr.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo Naive Bayes\n",
    "modelo_nb = MultinomialNB()\n",
    "modelo_nb.fit(x_treino_tfidf, y_treino)\n",
    "\n",
    "# Avaliar o modelo Naive Bayes\n",
    "y_teste_pred_nb = modelo_nb.predict(x_teste_tfidf)\n",
    "acuracia_nb = accuracy_score(y_teste, y_teste_pred_nb)\n",
    "precisao_nb = precision_score(y_teste, y_teste_pred_nb)\n",
    "recall_nb = recall_score(y_teste, y_teste_pred_nb)\n",
    "f1_nb = f1_score(y_teste, y_teste_pred_nb)\n",
    "\n",
    "print(f\"Acurácia Naive Bayes: {acuracia_nb}\")\n",
    "print(f\"Precisão Naive Bayes: {precisao_nb}\")\n",
    "print(f\"Recall Naive Bayes: {recall_nb}\")\n",
    "print(f\"F1 Score Naive Bayes: {f1_nb}\")\n",
    "\n",
    "# Matriz de Confusão Naive Bayes\n",
    "cm_nb = confusion_matrix(y_teste, y_teste_pred_nb)\n",
    "disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=rotulos_unicos)\n",
    "disp_nb.plot(cmap='tab20b')\n",
    "plt.title('Matriz de Confusão para Naive Bayes')\n",
    "plt.savefig('../images/matriz_confusao_naive_bayes.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores (Naive Bayes): [0.97222222 0.95833333 0.96906566 0.96906566 0.96906566]\n",
      "Mean CV Accuracy (Naive Bayes): 0.967550505050505\n"
     ]
    }
   ],
   "source": [
    "# Definir o modelo\n",
    "mod_nb = MultinomialNB()\n",
    "\n",
    "# Executar a validação cruzada\n",
    "cv_scores_nb = cross_val_score(mod_nb, x_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores (Naive Bayes): {cv_scores_nb}\")\n",
    "print(f\"Mean CV Accuracy (Naive Bayes): {cv_scores_nb.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redução de Dimensionalidade com TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=200)  # Ajustar o número de componentes conforme necessário\n",
    "x_treino_reduzido = svd.fit_transform(x_treino_tfidf)\n",
    "x_teste_reduzido = svd.transform(x_teste_tfidf)\n",
    "\n",
    "# Modelo 4: Linear Discriminant Analysis com Redução de Dimensionalidade\n",
    "# Treinar o modelo Linear Discriminant Analysis\n",
    "modelo_lda = LinearDiscriminantAnalysis()\n",
    "modelo_lda.fit(x_treino_reduzido, y_treino)\n",
    "\n",
    "# Avaliar o modelo Linear Discriminant Analysis\n",
    "y_teste_pred_lda = modelo_lda.predict(x_teste_reduzido)\n",
    "acuracia_lda = accuracy_score(y_teste, y_teste_pred_lda)\n",
    "precisao_lda = precision_score(y_teste, y_teste_pred_lda)\n",
    "recall_lda = recall_score(y_teste, y_teste_pred_lda)\n",
    "f1_lda = f1_score(y_teste, y_teste_pred_lda)\n",
    "\n",
    "print(f\"Acurácia Linear Discriminant Analysis: {acuracia_lda}\")\n",
    "print(f\"Precisão Linear Discriminant Analysis: {precisao_lda}\")\n",
    "print(f\"Recall Linear Discriminant Analysis: {recall_lda}\")\n",
    "print(f\"F1 Score Linear Discriminant Analysis: {f1_lda}\")\n",
    "\n",
    "# Matriz de Confusão Linear Discriminant Analysis\n",
    "cm_lda = confusion_matrix(y_teste, y_teste_pred_lda)\n",
    "disp_lda = ConfusionMatrixDisplay(confusion_matrix=cm_lda, display_labels=rotulos_unicos)\n",
    "disp_lda.plot(cmap='tab20b')\n",
    "plt.title('Matriz de Confusão para Linear Discriminant Analysis')\n",
    "# plt.savefig('../images/matriz_confusao_lda.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores (LDA): [0.99873737 0.99810606 0.99621212 0.99684343 0.99936869]\n",
      "Mean CV Accuracy (LDA): 0.9978535353535353\n"
     ]
    }
   ],
   "source": [
    "# Definir o modelo\n",
    "mod_lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Executar a validação cruzada\n",
    "cv_scores_lda = cross_val_score(mod_lda, x_train_tfidf.toarray(), y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores (LDA): {cv_scores_lda}\")\n",
    "print(f\"Mean CV Accuracy (LDA): {cv_scores_lda.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo K-Nearest Neighbors\n",
    "modelo_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "modelo_knn.fit(x_treino_tfidf.toarray(), y_treino)  # KNN precisa de matrizes densas\n",
    "\n",
    "# Avaliar o modelo K-Nearest Neighbors\n",
    "y_teste_pred_knn = modelo_knn.predict(x_teste_tfidf.toarray())\n",
    "acuracia_knn = accuracy_score(y_teste, y_teste_pred_knn)\n",
    "precisao_knn = precision_score(y_teste, y_teste_pred_knn)\n",
    "recall_knn = recall_score(y_teste, y_teste_pred_knn)\n",
    "f1_knn = f1_score(y_teste, y_teste_pred_knn)\n",
    "\n",
    "print(f\"Acurácia K-Nearest Neighbors: {acuracia_knn}\")\n",
    "print(f\"Precisão K-Nearest Neighbors: {precisao_knn}\")\n",
    "print(f\"Recall K-Nearest Neighbors: {recall_knn}\")\n",
    "print(f\"F1 Score K-Nearest Neighbors: {f1_knn}\")\n",
    "\n",
    "# Matriz de Confusão K-Nearest Neighbors\n",
    "cm_knn = confusion_matrix(y_teste, y_teste_pred_knn)\n",
    "disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=rotulos_unicos)\n",
    "disp_knn.plot(cmap='tab20b')\n",
    "plt.title('Matriz de Confusão para K-Nearest Neighbors')\n",
    "# plt.savefig('../images/matriz_confusao_knn.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores (KNN): [0.92739899 0.91035354 0.91224747 0.91540404 0.92424242]\n",
      "Mean CV Accuracy (KNN): 0.9179292929292929\n"
     ]
    }
   ],
   "source": [
    "# Definir o modelo\n",
    "mod_knn = KNeighborsClassifier()\n",
    "\n",
    "# Executar a validação cruzada\n",
    "cv_scores_knn = cross_val_score(mod_knn, x_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores (KNN): {cv_scores_knn}\")\n",
    "print(f\"Mean CV Accuracy (KNN): {cv_scores_knn.mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
